# Testing Guide for SAP AI Core LLM Proxy

## Overview

The test suite provides comprehensive coverage of [`proxy_server.py`](../proxy_server.py) with **50 passing tests** achieving **28% code coverage** (focused on critical business logic).

## Quick Start

```bash
# Install test dependencies
make install-test-deps

# Run all tests
make test

# Run with coverage report
make test-cov

# Run verbose
make test-verbose
```

## Test Results Summary

```
‚úÖ 50 tests passed
üìä 28% code coverage
‚è±Ô∏è  1.35s execution time
```

## Test Organization

### 1. Dataclass Tests (8 tests)
- [`ServiceKey`](../proxy_server.py:23) - Service key structure validation
- [`TokenInfo`](../proxy_server.py:31) - Token caching with threading
- [`SubAccountConfig`](../proxy_server.py:36) - Subaccount configuration and model normalization
- [`ProxyConfig`](../proxy_server.py:70) - Global configuration and model mapping

### 2. Model Detection Tests (14 tests)
- [`is_claude_model()`](../proxy_server.py:1064) - Detects Claude models (7 test cases)
- [`is_claude_37_or_4()`](../proxy_server.py:1067) - Detects Claude 3.7/4/4.5 (5 test cases)
- [`is_gemini_model()`](../proxy_server.py:1079) - Detects Gemini models (7 test cases)

### 3. Conversion Function Tests (6 tests)
- [`convert_openai_to_claude()`](../proxy_server.py:424) - OpenAI ‚Üí Claude format
- [`convert_openai_to_claude37()`](../proxy_server.py:444) - OpenAI ‚Üí Claude 3.7 format
- [`convert_claude_to_openai()`](../proxy_server.py:718) - Claude ‚Üí OpenAI format
- [`convert_claude37_to_openai()`](../proxy_server.py:769) - Claude 3.7 ‚Üí OpenAI format
- [`convert_openai_to_gemini()`](../proxy_server.py:1091) - OpenAI ‚Üí Gemini format
- [`convert_gemini_to_openai()`](../proxy_server.py:1230) - Gemini ‚Üí OpenAI format

### 4. Error Handling Tests (1 test)
- [`handle_http_429_error()`](../proxy_server.py:135) - HTTP 429 rate limit handling

### 5. Token Management Tests (7 tests)
- [`verify_request_token()`](../proxy_server.py:403) - Token verification (4 test cases)
- [`fetch_token()`](../proxy_server.py:322) - Token fetching with caching (3 test cases)

### 6. Load Balancing Tests (4 tests)
- [`load_balance_url()`](../proxy_server.py:1605) - Round-robin load balancing
- Single/multiple subaccount scenarios
- Model fallback mechanisms
- Error handling for missing models

### 7. Flask Endpoint Tests (3 tests)
- `/v1/models` - Model listing endpoint
- `/v1/embeddings` - Embeddings endpoint
- `/api/event_logging/batch` - Event logging endpoint

### 8. Configuration Tests (2 tests)
- [`load_config()`](../proxy_server.py:288) - Multi-subaccount format
- Legacy single-account format

### 9. Integration Tests (1 test)
- Complete chat completion workflow with mocked dependencies

## Test Commands

```bash
# Run all tests
make test

# Run with coverage (HTML report in htmlcov/)
make test-cov

# Run verbose mode
make test-verbose

# Run specific test file
make test-file FILE=tests/test_proxy_server.py

# Run tests matching pattern
make test-pattern PATTERN=token

# Run specific test class
pytest tests/test_proxy_server.py::TestTokenManagement -v

# Run specific test method
pytest tests/test_proxy_server.py::TestTokenManagement::test_verify_request_token_valid -v
```

## Coverage Report

Current coverage: **28%** (432/1526 statements)

### Covered Areas ‚úÖ
- Dataclass initialization and methods
- Model detection functions
- Payload conversion functions
- Token verification logic
- Load balancing with round-robin
- Configuration loading
- Basic Flask endpoint routing

### Not Covered (by design) ‚ö†Ô∏è
- Streaming response generators (requires complex mocking)
- SAP AI SDK integration (requires live credentials)
- Network error scenarios (requires integration tests)
- Flask app startup and main execution
- Token refresh on expiry (time-dependent)

## Test Fixtures

### Available Fixtures
- `sample_service_key` - Mock service key data
- `sample_config` - Mock proxy configuration
- `mock_service_key_file` - Temporary service key file
- `flask_client` - Flask test client
- `reset_proxy_config` - Resets global config between tests

### Using Fixtures

```python
def test_example(sample_service_key, flask_client):
    """Test using fixtures."""
    # sample_service_key and flask_client are automatically provided
    assert sample_service_key["clientid"] == "test-client-id"
    response = flask_client.get('/v1/models')
    assert response.status_code == 200
```

## Mocking Strategy

Tests use `unittest.mock` to isolate components:

```python
@patch('proxy_server.requests.post')
def test_with_mock(mock_post):
    """Test with mocked HTTP request."""
    mock_response = Mock()
    mock_response.json.return_value = {"result": "success"}
    mock_post.return_value = mock_response
    
    result = function_that_makes_request()
    assert result["result"] == "success"
```

## Adding New Tests

1. **Create test class** in [`tests/test_proxy_server.py`](../tests/test_proxy_server.py)
2. **Follow naming convention**: `test_*` for functions, `Test*` for classes
3. **Use fixtures** for common setup
4. **Mock external dependencies** (HTTP, file I/O, SDK calls)
5. **Add docstrings** explaining what is tested
6. **Run tests** to verify they pass

Example:

```python
class TestNewFeature:
    """Tests for new feature."""
    
    def test_basic_case(self):
        """Test basic functionality."""
        result = new_function("input")
        assert result == "expected"
    
    @pytest.mark.parametrize("input,expected", [
        ("a", "A"),
        ("b", "B"),
    ])
    def test_multiple_cases(self, input, expected):
        """Test with multiple inputs."""
        assert new_function(input) == expected
```

## CI/CD Integration

Tests run automatically in CI/CD pipelines via:

```bash
make build-tested  # Runs tests before building
```

## Troubleshooting

### Tests Not Running
```bash
# Ensure dependencies are installed
make install-test-deps
```

### Import Errors
```bash
# Reinstall in development mode
uv sync --extra dev
```

### Coverage Report Not Generated
```bash
# Install coverage tools
uv add --dev pytest-cov
```

## Best Practices

1. ‚úÖ **Test isolation** - Each test is independent
2. ‚úÖ **Clear naming** - Test names describe what they test
3. ‚úÖ **AAA pattern** - Arrange, Act, Assert
4. ‚úÖ **Mock externals** - No real HTTP calls or file I/O
5. ‚úÖ **Fast execution** - All tests run in ~1.3 seconds
6. ‚úÖ **Comprehensive** - Covers all critical business logic

## Future Enhancements

Potential areas for additional testing:

- [ ] Streaming response integration tests
- [ ] SAP AI SDK integration tests (with test credentials)
- [ ] Performance/load testing
- [ ] Error recovery scenarios
- [ ] Concurrent request handling
- [ ] Token refresh edge cases

## Resources

- [Pytest Documentation](https://docs.pytest.org/)
- [Test Suite README](../tests/README.md)
- [Pytest Configuration](../pytest.ini)