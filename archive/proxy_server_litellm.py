import logging
from flask import Flask, request, jsonify, Response, stream_with_context, Request # Added Request for type hinting
import requests # Keep for fetch_token AND manual Anthropic calls
import time
import threading
import json
import base64
import random
import os
import argparse
from typing import Optional, Dict, Any, Generator, Tuple

# Import Litellm (used only for Azure/OpenAI/Gemini models now)
import litellm
from litellm import ModelResponse
import traceback # For detailed error logging

# --- Flask App and Logging Setup ---
app = Flask(__name__)

# Configure basic logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - [%(threadName)s] - %(name)s - %(message)s')
# Silence overly verbose libraries
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("requests").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)


# --- Token Usage Logger Setup ---
token_logger = logging.getLogger('token_usage')
token_logger.setLevel(logging.INFO) # Log INFO level messages and above to the file
token_logger.propagate = False # Prevent double logging in the main console/logger
log_directory = 'logs'; os.makedirs(log_directory, exist_ok=True)
log_file = os.path.join(log_directory, 'token_usage.log')
try:
    # Use 'a+' to create if not exists, append otherwise
    file_handler = logging.FileHandler(log_file, mode='a+', encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - %(message)s')
    file_handler.setFormatter(formatter)
    token_logger.addHandler(file_handler)
    # --- Test Log Message ---
    token_logger.info("--- Token Logger Initialized ---")
    logging.info(f"Token usage logging configured to: {log_file}")
except Exception as e: # Catch broader exceptions during file handling setup
    logging.error(f"CRITICAL: Could not configure token usage log file {log_file}: {e}", exc_info=True)


# --- Global variables ---
backend_auth_token: Optional[str] = None
token_expiry_time: float = 0.0
token_fetch_lock = threading.Lock()

# --- Configuration Placeholders (loaded in __main__) ---
config: Dict[str, Any] = {}
service_key: Optional[Dict[str, str]] = None # REQUIRED for this setup
# This map MUST contain ALL supported models mapped to SAP AI Core deployment base URLs
aicore_deployment_urls: Dict[str, list[str]] = {}
secret_authentication_tokens: list[str] = []
resource_group: str = "" # REQUIRED for SAP AI Core Header


# --- Config Loading and Parsing ---
def load_config(file_path: str) -> Dict[str, Any]:
    """Loads configuration from a JSON file."""
    try:
        with open(file_path,'r', encoding='utf-8') as f: data=json.load(f); logging.info(f"Loaded config: {file_path}"); return data
    except FileNotFoundError: logging.critical(f"Config file not found: '{file_path}'"); raise
    except json.JSONDecodeError as e: logging.critical(f"Error decoding config JSON: '{file_path}': {e}"); raise
    except Exception as e: logging.critical(f"Unexpected error loading config '{file_path}': {e}"); raise

def parse_arguments() -> argparse.Namespace:
    """Parses command line arguments."""
    parser=argparse.ArgumentParser(description="SAP AI Core Hybrid Proxy v5 Complete"); parser.add_argument("--config",default="config.json"); return parser.parse_args()


# --- Backend Token Fetching (OAuth for SAP AI Core - MANDATORY) ---
def fetch_token() -> str:
    """Fetches or retrieves a cached SAP AI Core authentication token. Raises error on failure."""
    global backend_auth_token, token_expiry_time;
    if not service_key: raise ValueError("OAuth config missing");
    if not all(k in service_key for k in ['clientid', 'clientsecret', 'url']): raise ValueError("Service key config incomplete.")
    with token_fetch_lock:
        now=time.time();
        if backend_auth_token and now<token_expiry_time: logging.debug("Using cached token."); return backend_auth_token
        logging.info("Fetching new backend token for SAP AI Core.")
        try: # ...(fetch logic)...
            cid=service_key['clientid'];cs=service_key['clientsecret'];burl=service_key['url']; auth=f"{cid}:{cs}";enc=base64.b64encode(auth.encode('utf-8')).decode('ascii'); turl=f"{burl.rstrip('/')}/oauth/token?grant_type=client_credentials";h={"Authorization":f"Basic {enc}"}; res=requests.post(turl,headers=h,timeout=15); res.raise_for_status(); td=res.json(); nt=td.get('access_token'); exp_in=int(td.get('expires_in',14400));
            if not nt: raise ValueError("Fetched token empty")
            backend_auth_token=nt; token_expiry_time=now+exp_in-300; logging.info("Backend token fetched successfully.")
            return backend_auth_token
        except requests.exceptions.Timeout as err: logging.error(f"Timeout fetching token: {err}"); backend_auth_token=None; token_expiry_time=0; raise TimeoutError("Timeout connecting token endpoint") from err
        except requests.exceptions.HTTPError as err: logging.error(f"HTTP error fetching token: {err.response.status_code}-{err.response.text}"); backend_auth_token=None; token_expiry_time=0; raise ConnectionError(f"HTTP Error {err.response.status_code} fetching token") from err
        except requests.exceptions.RequestException as err: logging.error(f"Network/Request error fetching token: {err}"); backend_auth_token=None; token_expiry_time=0; raise ConnectionError(f"Network error fetching token: {err}") from err
        except Exception as err: logging.error(f"Unexpected token fetch error: {err}", exc_info=True); backend_auth_token=None; token_expiry_time=0; raise RuntimeError(f"Unexpected error processing token response: {err}") from err


# --- Client Request Token Verification ---
def verify_request_token(request: Request) -> bool:
    """Verifies the Authorization header from the incoming client request."""
    if not secret_authentication_tokens: logging.warning("Client Auth disabled."); return True
    ct = request.headers.get("Authorization"); log_prefix = ct[:15]+"..." if ct and len(ct)>15 else ct; logging.debug(f"Verifying client token prefix: '{log_prefix}'")
    if not ct or not any(sk in ct for sk in secret_authentication_tokens): logging.warning("Invalid/Missing client token."); return False
    logging.debug("Client token verified."); return True


# --- Load Balancing (Round-robin for SAP AI Core BASE URLs) ---
def load_balance_url(urls: list[str], model_key: str) -> Optional[str]:
    """Selects a BASE URL from a list using round-robin. Returns None if list is empty."""
    logging.debug(f"Load balancing for model_key: {model_key} with base urls: {urls}")
    if not urls: logging.error(f"No BASE URLs for AI Core model '{model_key}'."); return None
    if not hasattr(load_balance_url,"counters"): load_balance_url.counters={}
    if model_key not in load_balance_url.counters: load_balance_url.counters[model_key]=0
    with threading.Lock(): idx=load_balance_url.counters[model_key]%len(urls); url=urls[idx]; load_balance_url.counters[model_key]+=1
    logging.info(f"Selected AI Core base URL for '{model_key}': {url}")
    return url


# --- Helper to determine UNDERLYING provider type ---
def get_underlying_provider(model_name: str) -> str:
    """Determines the underlying LLM type (azure, anthropic, gemini) for SAP AI Core routing."""
    model_name_lower = model_name.lower() if model_name else ""
    if "claude" in model_name_lower or "sonnet" in model_name_lower: return "anthropic"
    if "gemini" in model_name_lower: return "gemini"
    logging.debug(f"Model '{model_name}' mapped to 'azure' underlying provider.")
    return "azure" # Default for gpt-* and others via AI Core


# --- Token Usage Logging Helper (Always Logs) ---
def log_token_usage(request: Request, model_name: str, usage_object: Optional[Dict[str, int]]):
    """Logs token usage details. Logs defaults (0) if data is missing/invalid."""
    log_prefix = f"[Token Log for {model_name}]"
    logging.debug(f"{log_prefix} Attempting log. Received usage object: {usage_object}")
    estimated = False
    try:
        prompt_tokens = 0; completion_tokens = 0; total_tokens = 0
        if usage_object and isinstance(usage_object, dict):
            try: prompt_tokens = int(usage_object.get('prompt_tokens', 0) or 0)
            except (ValueError, TypeError): logging.warning(f"{log_prefix} Invalid prompt_tokens: {usage_object.get('prompt_tokens')}")
            try: completion_tokens = int(usage_object.get('completion_tokens', 0) or 0)
            except (ValueError, TypeError): logging.warning(f"{log_prefix} Invalid completion_tokens: {usage_object.get('completion_tokens')}")
            if prompt_tokens > 0 or completion_tokens > 0:
                 total_tokens = usage_object.get('total_tokens', prompt_tokens + completion_tokens)
                 try: total_tokens = int(total_tokens or 0)
                 except (ValueError, TypeError): total_tokens = prompt_tokens + completion_tokens
            # Check if we defaulted genuinely zero values
            original_pt = usage_object.get('prompt_tokens'); original_ct = usage_object.get('completion_tokens')
            if prompt_tokens == 0 and completion_tokens == 0 and not (original_pt == 0 and original_ct == 0): estimated = True
        else: estimated = True; logging.warning(f"{log_prefix} Valid usage object not provided. Logging defaults.")
        # ...(Sanitize user id, get ip)...
        client_token = request.headers.get("Authorization", "unknown"); max_len=30; user_id="***"
        if client_token and client_token != "unknown":
             if client_token.lower().startswith("bearer "): token_part=client_token[7:]; user_id=f"Bearer {token_part[:8]}..." if len(token_part)>8 else f"Bearer {token_part}"
             else: user_id=f"{client_token[:max_len]}..." if len(client_token)>max_len else client_token
        ip_address = request.remote_addr or request.headers.get("X-Forwarded-For", "unknown_ip")
        log_message = (f"User: {user_id}, IP: {ip_address}, Model: {model_name}, PromptTokens: {prompt_tokens}, CompletionTokens: {completion_tokens}, TotalTokens: {total_tokens}{' (Estimated/Default)' if estimated else ''}")
        token_logger.info(log_message)
        logging.info(f"{log_prefix} Logged usage to token_usage.log {'(Estimated)' if estimated else ''}.")
    except Exception as e: logging.error(f"{log_prefix} Failed during token usage logging: {e}", exc_info=True)


# --- *** Manual Conversion Functions for Anthropic *** ---
def convert_openai_to_anthropic_payload(openai_payload: Dict[str, Any]) -> Dict[str, Any]:
    """Converts OpenAI format payload to Anthropic Messages API format for SAP AI Core."""
    logging.debug("Converting OpenAI payload to Anthropic format for AI Core")
    messages=openai_payload.get("messages",[]); system_prompt=""; processed_messages=[]
    if messages and messages[0].get("role") == "system": system_prompt=messages[0].get("content",""); messages=messages[1:]
    for msg in messages:
        role=msg.get("role"); content=msg.get("content","");
        if role in ["user","assistant"]: processed_messages.append({"role":role,"content":content})
        else: logging.warning(f"Ignoring message with unhandled role for Anthropic: {role}")
    anthropic_payload = {
        "anthropic_version": "bedrock-2023-05-31", # REQUIRED - Verify this value
        "messages": processed_messages,
        "max_tokens": openai_payload.get("max_tokens",4096),
        **{k_out: openai_payload[k_in] for k_in, k_out in {"temperature":"temperature", "top_p":"top_p", "top_k":"top_k", "stop":"stop_sequences"}.items() if k_in in openai_payload}
    }
    if system_prompt: anthropic_payload["system"] = system_prompt
    anthropic_payload = {k:v for k,v in anthropic_payload.items() if v is not None} # Clean None values
    logging.debug(f"Converted Anthropic payload: {json.dumps(anthropic_payload)}")
    return anthropic_payload

def convert_anthropic_chunk_to_openai(anthropic_json_str: str, model_name: str) -> Tuple[Optional[str], Optional[Dict[str, int]]]:
    """Converts Anthropic JSON data chunk string to OpenAI SSE format string. Returns (sse_string|None, usage_dict|None)."""
    logging.debug(f"Converting Anthropic JSON chunk: {anthropic_json_str}")
    usage_info: Dict[str,Any]={}; openai_sse_str: Optional[str]=None
    try: data=json.loads(anthropic_json_str); event_type=data.get("type"); assert event_type
    except: logging.error(f"Failed decoding/parsing Anthropic JSON: {anthropic_json_str}"); return None, None
    openai_chunk={"id":f"chatcmpl-anth-{int(time.time()*1000)}-{random.randint(100,999)}", "object":"chat.completion.chunk", "created":int(time.time()), "model":model_name, "choices":[{"index":0,"delta":{},"finish_reason":None}]}
    finish_reason=None; content_delta=None; role=None
    if event_type == "message_start":
        role=data.get("message",{}).get("role","assistant")
        if data.get("message", {}).get("usage"): usage_info['prompt_tokens'] = data["message"]["usage"].get("input_tokens")
    elif event_type == "content_block_delta":
        if data.get("delta",{}).get("type")=="text_delta": content_delta=data["delta"].get("text")
    elif event_type == "message_delta":
        stop_reason=data.get("delta",{}).get("stop_reason")
        if stop_reason: finish_reason = {"end_turn":"stop","max_tokens":"length","stop_sequence":"stop"}.get(stop_reason,"stop")
        if data.get("usage"): usage_info['completion_tokens']=data["usage"].get("output_tokens")
    elif event_type == "message_stop":
        bm=data.get("amazon-bedrock-invocationMetrics");
        if bm: usage_info['prompt_tokens']=bm.get("inputTokenCount"); usage_info['completion_tokens']=bm.get("outputTokenCount")
    if role: openai_chunk["choices"][0]["delta"]["role"]=role
    if content_delta: openai_chunk["choices"][0]["delta"]["content"]=content_delta
    if finish_reason: openai_chunk["choices"][0]["finish_reason"]=finish_reason
    if openai_chunk["choices"][0]["delta"] or finish_reason: openai_sse_str = f"data: {json.dumps(openai_chunk)}\n\n"
    final_usage = {k:v for k,v in usage_info.items() if v is not None}
    return openai_sse_str, final_usage if final_usage else None

def convert_anthropic_response_to_openai(anthropic_response_data: Dict[str, Any], client_model_name: str) -> Dict[str, Any]:
    """Converts a complete Anthropic Messages API JSON response to OpenAI format."""
    logging.debug("Converting full Anthropic response to OpenAI format")
    openai_response={"id":anthropic_response_data.get("id",f"chatcmpl-anth-{int(time.time())}"),"object":"chat.completion","created":int(time.time()),"model":client_model_name,"choices":[],"usage":{"prompt_tokens":0,"completion_tokens":0,"total_tokens":0}}
    content_text = ""
    if isinstance(anthropic_response_data.get("content"),list) and len(anthropic_response_data["content"])>0:
        block=anthropic_response_data["content"][0];
        if block.get("type")=="text": content_text=block.get("text","")
    anthropic_stop=anthropic_response_data.get("stop_reason"); openai_finish="stop"
    if anthropic_stop=="end_turn": openai_finish="stop"
    elif anthropic_stop=="max_tokens": openai_finish="length"
    elif anthropic_stop=="stop_sequence": openai_finish="stop"
    openai_response["choices"].append({"index":0,"message":{"role":anthropic_response_data.get("role","assistant"),"content":content_text},"finish_reason":openai_finish})
    usage=anthropic_response_data.get("usage")
    if isinstance(usage,dict):
        pt=int(usage.get("input_tokens",0) or 0); ct=int(usage.get("output_tokens",0) or 0)
        openai_response["usage"]={"prompt_tokens":pt,"completion_tokens":ct,"total_tokens":pt+ct}
    logging.debug(f"Converted OpenAI response: {json.dumps(openai_response)}")
    return openai_response


# --- Flask Routes ---

@app.route('/v1/chat/completions', methods=['OPTIONS'])
def handle_options():
    resp=jsonify({}); resp.headers.add('Access-Control-Allow-Origin','*'); resp.headers.add('Access-Control-Allow-Headers','Content-Type,Authorization'); resp.headers.add('Access-Control-Allow-Methods','POST,OPTIONS'); resp.headers.add('Access-Control-Max-Age','86400'); return resp, 204

@app.route('/v1/models', methods=['GET'])
def list_models_endpoint():
    if not verify_request_token(request): return jsonify({"error":"Unauthorized"}), 401
    models=[{"id":m,"object":"model","created":int(time.time()),"owned_by":"sap-ai-core"} for m in aicore_deployment_urls.keys()]
    return jsonify({"object": "list", "data": models}), 200

# Main Chat Completions Proxy Endpoint
@app.route('/v1/chat/completions', methods=['POST'])
def chat_completions_proxy_endpoint():
    endpoint = "/v1/chat/completions"; request_start_time = time.time(); logging.info(f"POST {endpoint}")
    if not verify_request_token(request): return jsonify({"error": "Invalid API Key"}), 401
    current_backend_token: Optional[str] = None
    try: current_backend_token = fetch_token()
    except Exception as e: logging.error(f"CRITICAL - Failed fetch token: {e}",exc_info=True); return jsonify({"error":"Proxy backend auth failed"}), 503
    try: request_data = request.get_json(); assert request_data
    except: return jsonify({"error": "Invalid JSON body"}), 400
    client_model_name: Optional[str] = request_data.get("model"); messages: Optional[list] = request_data.get("messages"); stream: bool = request_data.get("stream", False)
    if not client_model_name or not messages: return jsonify({"error": "Missing model or messages"}), 400
    if client_model_name not in aicore_deployment_urls: return jsonify({"error": f"Model '{client_model_name}' not configured."}), 404
    base_urls = aicore_deployment_urls[client_model_name]; selected_base_url = load_balance_url(base_urls, client_model_name)
    if not selected_base_url: return jsonify({"error": "Internal config error: No base URL."}), 500
    underlying_provider = get_underlying_provider(client_model_name)

    try: # Master try block
        if underlying_provider == "anthropic":
            # --- MANUAL ANTHROPIC ---
            logging.info(f"Handling '{client_model_name}' manually (Anthropic via AI Core)")
            path = "/invoke-with-response-stream" if stream else "/invoke"
            final_endpoint_url = f"{selected_base_url.rstrip('/')}{path}"; logging.info(f"AI Core Anthropic URL: {final_endpoint_url}")
            headers = {"Authorization": f"Bearer {current_backend_token}", "Content-Type": "application/json", "AI-Resource-Group": resource_group, "Accept": "text/event-stream" if stream else "application/json"}
            anthropic_payload = convert_openai_to_anthropic_payload(request_data); logging.debug(f"Manual Anthropic Payload: {json.dumps(anthropic_payload)}")
            response = requests.post( final_endpoint_url, headers=headers, json=anthropic_payload, stream=stream, timeout=600 ); logging.info(f"Manual request sent. Status: {response.status_code}"); response.raise_for_status()
            if stream: return Response(stream_with_context(generate_anthropic_stream(response, endpoint, client_model_name, request)), mimetype='text/event-stream')
            else: # Non-streaming manual Anthropic
                 logging.info(f"Processing non-streaming manual response for {client_model_name}")
                 try: anthropic_response_data = response.json(); logging.debug(f"Raw Anthropic response: {json.dumps(anthropic_response_data)}")
                 except json.JSONDecodeError as json_err: logging.error(f"Failed decoding Anthropic JSON: {json_err}. Text: {response.text[:500]}"); return jsonify({"error":"Failed parse backend response."}), 502
                 openai_response_dict = convert_anthropic_response_to_openai(anthropic_response_data, client_model_name)
                 log_token_usage(request, client_model_name, openai_response_dict.get('usage')) # Log from converted data
                 return jsonify(openai_response_dict)

        elif underlying_provider == "azure" or underlying_provider == "gemini":
            # --- LITELLM AZURE / GEMINI ---
            logging.info(f"Handling '{client_model_name}' using Litellm (ProviderHint=azure)")
            final_endpoint_url: Optional[str] = None; path="/chat/completions"; api_ver="2023-05-15"
            if underlying_provider=="gemini": api_ver="2023-12-01-preview" # Assumed
            elif "o3-mini" in client_model_name.lower(): api_ver="2024-12-01-preview"
            final_endpoint_url = f"{selected_base_url.rstrip('/')}{path}?api-version={api_ver}"; logging.info(f"AI Core Azure/Gemini URL: {final_endpoint_url}")
            litellm_params = { "model": client_model_name, "custom_llm_provider": "azure", "api_base": final_endpoint_url, "api_key": current_backend_token, "headers": { "AI-Resource-Group": resource_group }, "messages": messages, "stream": stream,
                              **{k: request_data[k] for k in ["temperature","top_p","max_tokens","n","stop","presence_penalty","frequency_penalty","user","logit_bias","logprobs","top_logprobs","tools","tool_choice","response_format"] if k in request_data} } # Optional params
            logging.info(f"Attempting Litellm call (Hint: azure) for '{client_model_name}'"); logging.debug(f"Litellm params: { {k: (v if k!='api_key' else '***') for k,v in litellm_params.items()} }")
            if stream:
                response_generator = litellm.completion(**litellm_params)
                return Response(stream_with_context(generate_litellm_stream_response(response_generator, endpoint, client_model_name, request)), mimetype='text/event-stream')
            else: # Non-streaming Litellm
                response: ModelResponse = litellm.completion(**litellm_params); response_dict = response.dict(); logging.debug("Litellm non-streaming response received.")
                log_token_usage(request, client_model_name, response_dict.get('usage')); return jsonify(response_dict)
        else: # Should not be reached
            logging.error(f"Internal Error: Unknown provider '{underlying_provider}'"); return jsonify({"error": "Internal config error."}), 500

    # --- Exception Handling ---
    except requests.exceptions.RequestException as e: # Catch manual request errors
         status_code = e.response.status_code if hasattr(e,'response') and e.response is not None else 503; error_msg = f"Conn/HTTP Err (Manual Path): {e}"; logging.error(f"{endpoint}: {error_msg}", exc_info=True); detail=None;
         if hasattr(e, 'response') and e.response is not None: logging.error(f"Backend Response: {e.response.text[:500]}"); detail={"raw":e.response.text[:500]}
         return jsonify({"error": {"message": error_msg, "backend_detail": detail}}), status_code
    except litellm.exceptions.AuthenticationError as e: logging.error(f"Litellm Auth Err: {e}",exc_info=True); return jsonify({"error":f"Backend auth failed: {e}"}), 500
    except litellm.exceptions.NotFoundError as e: logging.error(f"Litellm Not Found Err: {e}",exc_info=True); return jsonify({"error":f"Deployment/Model not found: {e}"}), 404
    except litellm.exceptions.RateLimitError as e: logging.error(f"Litellm Rate Limit Err: {e}",exc_info=True); return jsonify({"error":f"Rate limit hit: {e}"}), 429
    except litellm.exceptions.BadRequestError as e: logging.error(f"Litellm Bad Request Err: {e}",exc_info=True); return jsonify({"error":f"Invalid request for backend: {e}"}), 400
    except litellm.exceptions.APIConnectionError as e: logging.error(f"Litellm Connection Err: {e}",exc_info=True); return jsonify({"error":f"Connection error: {e}"}), 503
    except litellm.exceptions.Timeout as e: logging.error(f"Litellm Timeout Err: {e}",exc_info=True); return jsonify({"error":f"Timeout connecting: {e}"}), 504
    except litellm.exceptions.APIError as e: logging.error(f"Litellm API Err: Status={e.status_code}, Err={e}",exc_info=True); code=e.status_code if isinstance(e.status_code,int) and 400<=e.status_code<600 else 500; return jsonify({"error":f"API error: {e}"}), code
    except Exception as e: logging.error(f"{endpoint}: Unhandled Exception: {e}", exc_info=True); logging.error(traceback.format_exc()); return jsonify({"error":f"Internal proxy error: {e}"}), 500
    finally: # Corrected finally block
        duration = time.time()-request_start_time
        # Ensure client_model_name exists before logging
        model_logged = client_model_name if 'client_model_name' in locals() else "UNKNOWN_MODEL"
        provider_logged = underlying_provider if 'underlying_provider' in locals() else "UNKNOWN_PROVIDER"
        logging.info(f"{endpoint}: Request for '{model_logged}' completed. Provider='{provider_logged}'. Duration: {duration:.3f}s")


# --- Helper function for MANUAL Anthropic streaming ---
def generate_anthropic_stream(response: requests.Response, endpoint: str, client_model_name: str, flask_request: Request) -> Generator[bytes, None, None]:
    """Processes Anthropic stream, converts chunks, aggregates usage, yields OpenAI SSE format."""
    sse_buffer = ""; chunks_yielded = 0; final_usage_data={}; usage_found = False
    start_time = time.time(); logging.info(f"{endpoint}: Starting robust manual stream processing for {client_model_name}")
    try:
        for chunk in response.iter_content(chunk_size=512, decode_unicode=True):
            if not chunk: continue
            sse_buffer += chunk
            while '\n\n' in sse_buffer:
                msg_end_idx=sse_buffer.find('\n\n'); complete_msg=sse_buffer[:msg_end_idx+2]; sse_buffer=sse_buffer[msg_end_idx+2:]
                cleaned_msg = "\n".join(line.strip() for line in complete_msg.strip().split('\n'))
                json_start_idx = cleaned_msg.find("data: ")
                if json_start_idx != -1:
                     json_part = cleaned_msg[json_start_idx+len("data: "):].strip()
                     if json_part:
                          openai_sse_str, usage_info = convert_anthropic_chunk_to_openai(json_part, client_model_name)
                          if usage_info: logging.debug(f"Usage from Anthropic chunk: {usage_info}"); final_usage_data.update({k:v for k,v in usage_info.items() if v is not None}); usage_found=True
                          if openai_sse_str:
                              try: yield openai_sse_str.encode('utf-8'); chunks_yielded+=1
                              except: logging.error("Stream yield error"); raise StopIteration
        # Process remaining buffer
        if sse_buffer.strip():
            logging.warning(f"Processing remaining buffer content: {sse_buffer.strip()}")
            cleaned_final_message = "\n".join(line.strip() for line in sse_buffer.strip().split('\n'))
            json_start_index = cleaned_final_message.find("data: ")
            if json_start_index != -1:
                json_part = cleaned_final_message[json_start_index + len("data: "):].strip()
                if json_part:
                    openai_sse_str, usage_info = convert_anthropic_chunk_to_openai(json_part, client_model_name)
                    if usage_info: final_usage_data.update({k:v for k,v in usage_info.items() if v is not None}); usage_found = True
                    if openai_sse_str:
                        try: yield openai_sse_str.encode('utf-8'); chunks_yielded += 1
                        except Exception as yield_err: logging.error(f"Stream yield error on final buffer: {yield_err}")
        yield "data: [DONE]\n\n".encode('utf-8')
        logging.info(f"{endpoint}: Manual stream finished for {client_model_name}. Chunks: {chunks_yielded}. Time: {time.time()-start_time:.2f}s")
    except requests.exceptions.ChunkedEncodingError as cee: logging.error(f"ChunkedEncodingError: {cee}"); #...(yield error chunk)...
    except Exception as e: logging.error(f"Manual stream error: {e}", exc_info=True); #...(yield error chunk)...
    finally:
        response.close()
        logging.debug(f"[Anthropic Stream Finally] Final Usage Data: {final_usage_data}, Usage Found Flag: {usage_found}")
        # ** Always call log_token_usage **
        if 'total_tokens' not in final_usage_data: final_usage_data["total_tokens"] = final_usage_data.get("prompt_tokens", 0) + final_usage_data.get("completion_tokens", 0)
        log_token_usage(flask_request, client_model_name, final_usage_data if final_usage_data else {}) # Pass empty dict if None/empty


# --- Helper function for Litellm streaming response generation ---
def generate_litellm_stream_response(response_generator: Generator, endpoint: str, client_model_name: str, flask_request: Request) -> Generator[bytes, None, None]:
    """Generator function to yield Litellm SSE formatted chunks and log usage."""
    final_usage_data = None; start_time = time.time(); chunks_yielded = 0; usage_found = False
    logging.info(f"{endpoint}: Litellm stream starting for {client_model_name}")
    try:
        for chunk in response_generator:
            try: chunk_dict = chunk.dict()
            except: logging.error("Litellm stream chunk format error"); continue
            if chunk_dict.get('usage'): final_usage_data = chunk_dict['usage']; usage_found = True; logging.debug(f"Litellm usage chunk: {final_usage_data}")
            try: yield f"data: {json.dumps(chunk_dict)}\n\n".encode('utf-8'); chunks_yielded+=1
            except: logging.error("Stream yield error"); break
        yield "data: [DONE]\n\n".encode('utf-8')
        logging.info(f"{endpoint}: Litellm stream finished for {client_model_name}. Chunks: {chunks_yielded}. Time: {time.time()-start_time:.2f}s")
    except Exception as e: logging.error(f"{endpoint}: Litellm stream error: {e}", exc_info=True); #...(yield error chunk)...
    finally:
        logging.debug(f"[Litellm Stream Finally] Final Usage Data: {final_usage_data}, Usage Found Flag: {usage_found}")
        # ** Always call log_token_usage **
        if final_usage_data and 'total_tokens' not in final_usage_data: final_usage_data["total_tokens"] = final_usage_data.get("prompt_tokens", 0) + final_usage_data.get("completion_tokens", 0)
        log_token_usage(flask_request, client_model_name, final_usage_data if final_usage_data else {}) # Pass empty dict if None


# --- Main Execution Block ---
if __name__ == '__main__':
    args = parse_arguments()
    logging.info(f"--- Starting SAP AI Core Universal LLM Proxy v8 (Hybrid + Guaranteed Log) ---")
    logging.info(f"Loading configuration from: {args.config}")
    try: config = load_config(args.config)
    except: logging.critical("Exiting: Config loading failure."); exit(1)
    # ...(Load config values and validate)...
    service_key_json_path=config.get('service_key_json'); aicore_deployments_config=config.get('deployment_models',{}); secret_authentication_tokens=config.get('secret_authentication_tokens',[]); resource_group=config.get('resource_group',''); host=config.get('host','0.0.0.0'); port=config.get('port',3001); crit_err=False
    if not aicore_deployments_config: logging.critical("Config Error: 'deployment_models' empty."); crit_err=True
    if not service_key_json_path: logging.critical("Config Error: 'service_key_json' missing."); crit_err=True
    if not resource_group: logging.critical("Config Error: 'resource_group' missing."); crit_err=True
    if not isinstance(secret_authentication_tokens,list): logging.critical("Config Error: 'secret_authentication_tokens' not list."); crit_err=True
    if not secret_authentication_tokens: logging.warning("PROXY SECURITY: Client auth disabled.")
    aicore_deployment_urls = {} # Validate URLs
    for model, urls in aicore_deployments_config.items():
        if not isinstance(urls, list) or not urls: logging.critical(f"Config Error: URLs for '{model}' invalid."); crit_err=True; continue
        for i, url in enumerate(urls):
             if not isinstance(url,str) or not url.startswith(('http://','https://')): logging.critical(f"Config Error: Invalid URL '{url}' for model '{model}'."); crit_err=True
        if not crit_err: aicore_deployment_urls[model] = urls
    if crit_err: logging.critical("Exiting due to critical config errors."); exit(1)
    try: service_key = load_config(service_key_json_path); assert isinstance(service_key,dict) and all(k in service_key for k in ['clientid','clientsecret','url'])
    except Exception: logging.critical(f"Exiting: Failed loading/validating service key."); exit(1)
    logging.info(f"Proxy configured for SAP AI Core models: {list(aicore_deployment_urls.keys())}")
    # --- Start Server ---
    logging.info(f"Starting Server on http://{host}:{port}")
    # try: from waitress import serve; logging.info("Using Waitress."); serve(app, host=host, port=port, threads=10)
    # except ImportError: logging.warning("Waitress not found. Using Flask dev server."); 
    app.run(host=host, port=port, debug=False)